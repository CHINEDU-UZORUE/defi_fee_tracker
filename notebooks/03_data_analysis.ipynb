{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a837cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Solana DeFi Tracker - Analysis for Streamlit\n",
      "Processing data for Streamlit application\n",
      "Analysis timestamp: 2025-09-01 17:54:00\n",
      "\n",
      "ğŸ” Loading Collected Data...\n",
      "âœ… Loaded TVL data: solana_defi_tvl_20250901_163640.joblib\n",
      "âœ… Loaded TVL data (Shape: (250, 14))\n",
      "Columns: ['name', 'slug', 'tvl', 'chains', 'category', 'change_1h', 'change_1d', 'change_7d', 'mcap', 'symbol', 'url', 'description', 'gecko_id', 'timestamp']\n",
      "âœ… Loaded Revenue data: solana_revenue_20250901_164725.joblib\n",
      "âœ… Loaded Revenue data (Shape: (117, 7))\n",
      "Columns: ['protocol', 'total_24h', 'total_7d', 'total_30d', 'total_all_time', 'chain', 'timestamp']\n",
      "âœ… Loaded Fees data: solana_fees_20250901_164810.joblib\n",
      "âœ… Converted Fees data to DataFrame (Shape: (117, 7))\n",
      "Columns: ['protocol', 'total_24h', 'total_7d', 'total_30d', 'total_all_time', 'chain', 'timestamp']\n",
      "âœ… Loaded CoinGecko data: solana_coingecko_enhanced_20250901_165439.joblib\n",
      "â„¹ï¸ CoinGecko data is a dict (keeping as-is for specialized handling)\n",
      "âœ… Converted CoinGecko dict to DataFrame (Shape: (80, 20))\n",
      "âœ… Loaded Token Holders data: solana_token_holders_20250901_165835.joblib\n",
      "âœ… Loaded Token Holders data (Shape: (626, 10))\n",
      "Columns: ['token_name', 'token_symbol', 'token_address', 'rank', 'account_address', 'ui_amount', 'raw_amount', 'decimals', 'timestamp', 'percentage_of_top10']\n",
      "\n",
      "ğŸ”„ Processing and Consolidating Data...\n",
      "ğŸ“Š Base TVL data: 250 protocols\n",
      "âœ… Merged revenue data: 117 rows available\n",
      "âœ… Merged fees data: 117 rows available\n",
      "âœ… Merged CoinGecko data: 80 rows available\n",
      "ğŸ“Š Final consolidated dataset: (250, 34)\n",
      "ğŸ“‹ Tab 1 (Overview) dataset: (250, 8)\n",
      "ğŸ“‹ Tab 2 (Financial) dataset: (90, 12)\n",
      "ğŸ“Š Processing token holders data: (626, 10)\n",
      "âœ… Processed concentration metrics for 68 tokens\n",
      "ğŸ“‹ Tab 3 (Token Distribution) dataset: (68, 8)\n",
      "\n",
      "ğŸ“ˆ Creating Analysis Summaries...\n",
      "ğŸ“Š Category analysis: 39 categories\n",
      "ğŸ“Š Created 4 financial ranking tables\n",
      "ğŸ“Š Token distribution summary: 68 tokens analyzed\n",
      "\n",
      "ğŸ’¾ Saving Streamlit-Ready Datasets...\n",
      "ğŸ’¾ Saved tab1_overview: ..\\data\\streamlit\\tab1_overview_20250901_175401.joblib (Shape: (250, 8))\n",
      "ğŸ“„ Saved CSV: ..\\data\\streamlit\\tab1_overview_20250901_175401.csv\n",
      "ğŸ’¾ Saved tab2_financial: ..\\data\\streamlit\\tab2_financial_20250901_175401.joblib (Shape: (90, 12))\n",
      "ğŸ“„ Saved CSV: ..\\data\\streamlit\\tab2_financial_20250901_175401.csv\n",
      "ğŸ’¾ Saved tab3_distribution: ..\\data\\streamlit\\tab3_distribution_20250901_175401.joblib (Shape: (68, 9))\n",
      "ğŸ“„ Saved CSV: ..\\data\\streamlit\\tab3_distribution_20250901_175401.csv\n",
      "ğŸ’¾ Saved category_analysis: ..\\data\\streamlit\\category_analysis_20250901_175401.joblib (Shape: (39, 9))\n",
      "ğŸ“„ Saved CSV: ..\\data\\streamlit\\category_analysis_20250901_175401.csv\n",
      "ğŸ’¾ Saved financial_rankings: ..\\data\\streamlit\\financial_rankings_20250901_175401.joblib (Dict with 4 items)\n",
      "ğŸ’¾ Saved distribution_summary: ..\\data\\streamlit\\distribution_summary_20250901_175401.joblib (Shape: (3, 5))\n",
      "ğŸ“„ Saved CSV: ..\\data\\streamlit\\distribution_summary_20250901_175401.csv\n",
      "ğŸ’¾ Saved concentration_distribution: ..\\data\\streamlit\\concentration_distribution_20250901_175401.joblib (Shape: (4, 2))\n",
      "ğŸ“„ Saved CSV: ..\\data\\streamlit\\concentration_distribution_20250901_175401.csv\n",
      "ğŸ’¾ Saved full_dataset: ..\\data\\streamlit\\full_dataset_20250901_175401.joblib (Shape: (250, 34))\n",
      "ğŸ“„ Saved CSV: ..\\data\\streamlit\\full_dataset_20250901_175401.csv\n",
      "\n",
      "ğŸ¯ Creating Summary Statistics for Streamlit...\n",
      "ğŸ’¾ Saved summary statistics: ..\\data\\streamlit\\summary_stats_20250901_175401.joblib\n",
      "\n",
      "ğŸ“Š Data Processing Complete!\n",
      "============================================================\n",
      "Ready for Streamlit Application:\n",
      "  ğŸ“ Data directory: ..\\data\\streamlit\n",
      "  ğŸ“Š Tab 1 (Overview): (250, 8)\n",
      "  ğŸ’° Tab 2 (Financial): (90, 12)\n",
      "  ğŸ¯ Tab 3 (Distribution): (68, 8)\n",
      "\n",
      "ğŸ“ˆ Key Metrics Summary:\n",
      "  â€¢ Total Protocols: 250\n",
      "  â€¢ Total TVL: $76.74B\n",
      "  â€¢ Total Market Cap: $10.36B\n",
      "  â€¢ Daily Revenue: $7.63M\n",
      "  â€¢ Daily Fees: $7.63M\n",
      "  â€¢ Tokens Analyzed: 68\n",
      "  â€¢ Avg Gini Coefficient: 0.561\n",
      "\n",
      "ğŸš€ Data is ready for Streamlit dashboard implementation!\n",
      "ğŸ’¾ Saved metadata: ..\\data\\streamlit\\latest_data_metadata.joblib\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Notebook 3: Data Analysis for Streamlit Application\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import sys\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Set up paths\n",
    "parent_dir = str(Path().resolve().parent)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Set up directories\n",
    "processed_dir = os.path.normpath('../data/processed')\n",
    "api_responses_dir = os.path.normpath('../data/api_responses')\n",
    "streamlit_data_dir = os.path.normpath('../data/streamlit')\n",
    "\n",
    "# Create streamlit data directory\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "os.makedirs(streamlit_data_dir, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“Š Solana DeFi Tracker - Analysis for Streamlit\")\n",
    "print(f\"Processing data for Streamlit application\")\n",
    "print(f\"Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "def load_latest_file(directory, pattern):\n",
    "    \"\"\"Load the most recent file matching the pattern\"\"\"\n",
    "    files = glob.glob(os.path.join(directory, pattern))\n",
    "    if not files:\n",
    "        return None, None\n",
    "    latest_file = max(files, key=os.path.getctime)\n",
    "    return joblib.load(latest_file), latest_file\n",
    "\n",
    "def load_and_convert_data(directory, pattern, data_name):\n",
    "    \"\"\"Load and convert data to DataFrame if necessary\"\"\"\n",
    "    data, filepath = load_latest_file(directory, pattern)\n",
    "    if data is not None:\n",
    "        print(f\"âœ… Loaded {data_name}: {os.path.basename(filepath)}\")\n",
    "        # Keep dicts intact (e.g., CoinGecko enhanced data)\n",
    "        if isinstance(data, dict):\n",
    "            print(f\"â„¹ï¸ {data_name} is a dict (keeping as-is for specialized handling)\")\n",
    "            return data, filepath\n",
    "        if isinstance(data, list):\n",
    "            try:\n",
    "                data = pd.DataFrame(data)\n",
    "                print(f\"âœ… Converted {data_name} to DataFrame (Shape: {data.shape})\")\n",
    "                print(f\"Columns: {data.columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error converting {data_name} to DataFrame: {e}\")\n",
    "                data = pd.DataFrame()\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            print(f\"âœ… Loaded {data_name} (Shape: {data.shape})\")\n",
    "            print(f\"Columns: {data.columns.tolist()}\")\n",
    "        else:\n",
    "            print(f\"âŒ Unexpected {data_name} type: {type(data)}\")\n",
    "            data = pd.DataFrame()\n",
    "    else:\n",
    "        print(f\"âŒ No {data_name} found\")\n",
    "        data = pd.DataFrame()\n",
    "    return data, filepath\n",
    "\n",
    "def format_currency(amount):\n",
    "    \"\"\"Format currency amount with appropriate units\"\"\"\n",
    "    if amount is None or amount == 0:\n",
    "        return \"$0\"\n",
    "    if amount >= 1_000_000_000:\n",
    "        return f\"${amount/1_000_000_000:.2f}B\"\n",
    "    elif amount >= 1_000_000:\n",
    "        return f\"${amount/1_000_000:.2f}M\"\n",
    "    elif amount >= 1_000:\n",
    "        return f\"${amount/1_000:.2f}K\"\n",
    "    else:\n",
    "        return f\"${amount:.2f}\"\n",
    "\n",
    "print(\"\\nğŸ” Loading Collected Data...\")\n",
    "\n",
    "# Load all data with type checking\n",
    "tvl_data, tvl_file = load_and_convert_data(api_responses_dir, 'solana_defi_tvl_*.joblib', 'TVL data')\n",
    "revenue_data, revenue_file = load_and_convert_data(api_responses_dir, 'solana_revenue_*.joblib', 'Revenue data')\n",
    "fees_data, fees_file = load_and_convert_data(api_responses_dir, 'solana_fees_*.joblib', 'Fees data')\n",
    "coingecko_data, coingecko_file = load_and_convert_data(api_responses_dir, 'solana_coingecko_enhanced_*.joblib', 'CoinGecko data')\n",
    "\n",
    "# Handle CoinGecko data specifically (supports dict/list/df)\n",
    "coingecko_df = pd.DataFrame()\n",
    "if coingecko_data is not None:\n",
    "    if isinstance(coingecko_data, dict):\n",
    "        # Expect keys per protocol; convert to rows\n",
    "        coingecko_df = pd.DataFrame.from_dict(coingecko_data, orient='index').reset_index()\n",
    "        coingecko_df.rename(columns={'index': 'protocol_key'}, inplace=True)\n",
    "        print(f\"âœ… Converted CoinGecko dict to DataFrame (Shape: {coingecko_df.shape})\")\n",
    "    elif isinstance(coingecko_data, list):\n",
    "        coingecko_df = pd.DataFrame(coingecko_data)\n",
    "        print(f\"âœ… Converted CoinGecko list to DataFrame (Shape: {coingecko_df.shape})\")\n",
    "    elif isinstance(coingecko_data, pd.DataFrame):\n",
    "        coingecko_df = coingecko_data.copy()\n",
    "        print(f\"âœ… CoinGecko DataFrame shape: {coingecko_df.shape}\")\n",
    "    else:\n",
    "        print(\"âŒ Unsupported CoinGecko data type after load; skipping.\")\n",
    "else:\n",
    "    print(\"âŒ No CoinGecko data found\")\n",
    "\n",
    "holders_data, holders_file = load_and_convert_data(api_responses_dir, 'solana_token_holders_*.joblib', 'Token Holders data')\n",
    "\n",
    "print(\"\\nğŸ”„ Processing and Consolidating Data...\")\n",
    "\n",
    "def consolidate_protocol_data():\n",
    "    \"\"\"Consolidate all collected data into unified DataFrames for Streamlit\"\"\"\n",
    "    if isinstance(tvl_data, pd.DataFrame) and not tvl_data.empty:\n",
    "        df_base = tvl_data.copy()\n",
    "    else:\n",
    "        print(\"âŒ No TVL data to process\")\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Normalized key\n",
    "    df_base['protocol_clean'] = df_base['name'].astype(str).str.lower().str.replace(' ', '_')\n",
    "\n",
    "    print(f\"ğŸ“Š Base TVL data: {len(df_base)} protocols\")\n",
    "\n",
    "    # ---- Revenue (rename before merging)\n",
    "    if isinstance(revenue_data, pd.DataFrame) and not revenue_data.empty:\n",
    "        revenue_clean = revenue_data.copy()\n",
    "        if 'protocol' in revenue_clean.columns:\n",
    "            revenue_clean['protocol_clean'] = revenue_clean['protocol'].astype(str).str.lower().str.replace(' ', '_')\n",
    "        elif 'protocol_name' in revenue_clean.columns:\n",
    "            revenue_clean['protocol_clean'] = revenue_clean['protocol_name'].astype(str).str.lower().str.replace(' ', '_')\n",
    "        else:\n",
    "            # fallback: try name\n",
    "            revenue_clean['protocol_clean'] = revenue_clean.get('name', '').astype(str).str.lower().str.replace(' ', '_')\n",
    "\n",
    "        # rename columns up-front\n",
    "        rename_rev = {}\n",
    "        if 'total_24h' in revenue_clean.columns: rename_rev['total_24h'] = 'revenue_24h'\n",
    "        if 'total_7d' in revenue_clean.columns:  rename_rev['total_7d']  = 'revenue_7d'\n",
    "        if 'total_30d' in revenue_clean.columns: rename_rev['total_30d'] = 'revenue_30d'\n",
    "        revenue_clean.rename(columns=rename_rev, inplace=True)\n",
    "\n",
    "        cols_rev = ['protocol_clean'] + [c for c in ['revenue_24h','revenue_7d','revenue_30d'] if c in revenue_clean.columns]\n",
    "        df_base = df_base.merge(revenue_clean[cols_rev].drop_duplicates('protocol_clean'), on='protocol_clean', how='left')\n",
    "        print(f\"âœ… Merged revenue data: {revenue_clean.shape[0]} rows available\")\n",
    "\n",
    "    # ---- Fees (rename before merging)\n",
    "    if isinstance(fees_data, pd.DataFrame) and not fees_data.empty:\n",
    "        fees_clean = fees_data.copy()\n",
    "        if 'protocol' in fees_clean.columns:\n",
    "            fees_clean['protocol_clean'] = fees_clean['protocol'].astype(str).str.lower().str.replace(' ', '_')\n",
    "        elif 'protocol_name' in fees_clean.columns:\n",
    "            fees_clean['protocol_clean'] = fees_clean['protocol_name'].astype(str).str.lower().str.replace(' ', '_')\n",
    "        else:\n",
    "            fees_clean['protocol_clean'] = fees_clean.get('name', '').astype(str).str.lower().str.replace(' ', '_')\n",
    "\n",
    "        # rename columns up-front\n",
    "        rename_fees = {}\n",
    "        if 'total_24h' in fees_clean.columns: rename_fees['total_24h'] = 'fees_24h'\n",
    "        if 'total_7d' in fees_clean.columns:  rename_fees['total_7d']  = 'fees_7d'\n",
    "        if 'total_30d' in fees_clean.columns: rename_fees['total_30d'] = 'fees_30d'\n",
    "        fees_clean.rename(columns=rename_fees, inplace=True)\n",
    "\n",
    "        cols_fees = ['protocol_clean'] + [c for c in ['fees_24h','fees_7d','fees_30d'] if c in fees_clean.columns]\n",
    "        df_base = df_base.merge(fees_clean[cols_fees].drop_duplicates('protocol_clean'), on='protocol_clean', how='left')\n",
    "        print(f\"âœ… Merged fees data: {fees_clean.shape[0]} rows available\")\n",
    "\n",
    "    # ---- CoinGecko (market/price/volume/rank)\n",
    "    if not coingecko_df.empty:\n",
    "        cg = coingecko_df.copy()\n",
    "        # derive protocol_clean\n",
    "        if 'protocol_name' in cg.columns:\n",
    "            cg['protocol_clean'] = cg['protocol_name'].astype(str).str.lower().str.replace(' ', '_')\n",
    "        elif 'protocol_key' in cg.columns:\n",
    "            cg['protocol_clean'] = cg['protocol_key'].astype(str).str.lower().str.replace(' ', '_')\n",
    "        elif 'name' in cg.columns:\n",
    "            cg['protocol_clean'] = cg['name'].astype(str).str.lower().str.replace(' ', '_')\n",
    "        else:\n",
    "            # if no obvious key, skip merge\n",
    "            cg['protocol_clean'] = np.nan\n",
    "\n",
    "        keep_cols = ['protocol_clean','symbol','current_price_usd','market_cap_usd',\n",
    "                     'total_volume_24h_usd','price_change_24h_percent','price_change_7d_percent',\n",
    "                     'price_change_30d_percent','circulating_supply','total_supply','market_cap_rank']\n",
    "        keep_cols = [c for c in keep_cols if c in cg.columns]\n",
    "        if 'protocol_clean' in keep_cols:\n",
    "            df_base = df_base.merge(cg[keep_cols].dropna(subset=['protocol_clean']).drop_duplicates('protocol_clean'),\n",
    "                                    on='protocol_clean', how='left')\n",
    "            print(f\"âœ… Merged CoinGecko data: {cg.shape[0]} rows available\")\n",
    "        else:\n",
    "            print(\"âš ï¸ CoinGecko data missing protocol key; skipped merge\")\n",
    "\n",
    "    # ---- Derived metrics (safe)\n",
    "    df_base['revenue_24h']    = df_base.get('revenue_24h', 0).fillna(0)\n",
    "    df_base['fees_24h']       = df_base.get('fees_24h', 0).fillna(0)\n",
    "    df_base['market_cap_usd'] = df_base.get('market_cap_usd', 0).fillna(0)\n",
    "    df_base['tvl']            = df_base.get('tvl', 0).fillna(0)\n",
    "\n",
    "    # Price-to-Fees ratio (annualized)\n",
    "    df_base['pf_ratio'] = np.where(\n",
    "        df_base['fees_24h'] > 0,\n",
    "        df_base['market_cap_usd'] / (df_base['fees_24h'] * 365),\n",
    "        np.nan\n",
    "    )\n",
    "    # Price-to-Revenue ratio (annualized)\n",
    "    df_base['pr_ratio'] = np.where(\n",
    "        df_base['revenue_24h'] > 0,\n",
    "        df_base['market_cap_usd'] / (df_base['revenue_24h'] * 365),\n",
    "        np.nan\n",
    "    )\n",
    "    # Market Cap to TVL ratio\n",
    "    df_base['mcap_tvl_ratio'] = np.where(\n",
    "        df_base['tvl'] > 0,\n",
    "        df_base['market_cap_usd'] / df_base['tvl'],\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ“Š Final consolidated dataset: {df_base.shape}\")\n",
    "    return df_base\n",
    "\n",
    "def process_token_holders_data():\n",
    "    \"\"\"Process token holders data for concentration analysis\"\"\"\n",
    "    if not (isinstance(holders_data, pd.DataFrame) and not holders_data.empty):\n",
    "        print(\"âŒ No token holders data available\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"ğŸ“Š Processing token holders data: {holders_data.shape}\")\n",
    "    # Ensure sorted by rank so head() picks top holders\n",
    "    token_df_sorted = holders_data.sort_values(['token_name','rank'], ascending=[True, True]).copy()\n",
    "\n",
    "    concentration_metrics = []\n",
    "    for token_name in token_df_sorted['token_name'].unique():\n",
    "        token_df = token_df_sorted[token_df_sorted['token_name'] == token_name]\n",
    "        if len(token_df) > 0:\n",
    "            total_supply = token_df['ui_amount'].sum()\n",
    "            top_1_share  = (token_df.iloc[0]['ui_amount'] / total_supply * 100) if total_supply > 0 else 0\n",
    "            top_5_share  = (token_df.head(5)['ui_amount'].sum() / total_supply * 100) if total_supply > 0 else 0\n",
    "            top_10_share = (token_df.head(10)['ui_amount'].sum() / total_supply * 100) if total_supply > 0 else 0\n",
    "\n",
    "            # Gini coefficient\n",
    "            balances = token_df['ui_amount'].values\n",
    "            balances = balances[balances > 0]\n",
    "            if len(balances) > 1:\n",
    "                n = len(balances)\n",
    "                mean_balance = np.mean(balances)\n",
    "                if mean_balance > 0:\n",
    "                    diffs = np.abs(balances.reshape(-1,1) - balances.reshape(1,-1))\n",
    "                    gini = diffs.sum() / (2 * n * n * mean_balance)\n",
    "                else:\n",
    "                    gini = 0\n",
    "            else:\n",
    "                gini = 0\n",
    "\n",
    "            concentration_metrics.append({\n",
    "                'token_name': token_name,\n",
    "                'token_symbol': token_df.iloc[0].get('token_symbol', ''),\n",
    "                'total_accounts_analyzed': len(token_df),\n",
    "                'top_1_holder_share': top_1_share,\n",
    "                'top_5_holders_share': top_5_share,\n",
    "                'top_10_holders_share': top_10_share,\n",
    "                'gini_coefficient': gini,\n",
    "                'largest_holder_amount': token_df.iloc[0]['ui_amount'] if len(token_df) > 0 else 0\n",
    "            })\n",
    "\n",
    "    concentration_df = pd.DataFrame(concentration_metrics)\n",
    "    print(f\"âœ… Processed concentration metrics for {len(concentration_df)} tokens\")\n",
    "    return concentration_df\n",
    "\n",
    "def create_streamlit_datasets():\n",
    "    \"\"\"Create optimized datasets for each Streamlit tab\"\"\"\n",
    "    df_overview = consolidate_protocol_data()\n",
    "\n",
    "    # Tab 1: Protocol Overview (TVL, Market Cap, Categories)\n",
    "    if not df_overview.empty:\n",
    "        overview_cols = [\n",
    "            'name', 'category', 'tvl', 'market_cap_usd', 'current_price_usd',\n",
    "            'change_1d', 'price_change_24h_percent', 'mcap_tvl_ratio'\n",
    "        ]\n",
    "        # ensure columns exist\n",
    "        for c in ['market_cap_usd','current_price_usd','change_1d','price_change_24h_percent','mcap_tvl_ratio']:\n",
    "            if c not in df_overview.columns:\n",
    "                df_overview[c] = np.nan\n",
    "\n",
    "        df_tab1 = df_overview[overview_cols].copy()\n",
    "        df_tab1.rename(columns={\n",
    "            'name': 'Protocol',\n",
    "            'category': 'Category',\n",
    "            'tvl': 'TVL_USD',\n",
    "            'market_cap_usd': 'Market_Cap_USD',\n",
    "            'current_price_usd': 'Price_USD',\n",
    "            'change_1d': 'TVL_Change_1d',\n",
    "            'price_change_24h_percent': 'Price_Change_24h',\n",
    "            'mcap_tvl_ratio': 'MCap_TVL_Ratio'\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Fill NaN values\n",
    "        df_tab1['Market_Cap_USD'] = df_tab1['Market_Cap_USD'].fillna(0)\n",
    "        df_tab1['Price_USD'] = df_tab1['Price_USD'].fillna(0)\n",
    "        df_tab1['TVL_Change_1d'] = df_tab1['TVL_Change_1d'].fillna(0)\n",
    "        df_tab1['Price_Change_24h'] = df_tab1['Price_Change_24h'].fillna(0)\n",
    "\n",
    "        # Sort by TVL\n",
    "        df_tab1 = df_tab1.sort_values('TVL_USD', ascending=False).reset_index(drop=True)\n",
    "        print(f\"ğŸ“‹ Tab 1 (Overview) dataset: {df_tab1.shape}\")\n",
    "    else:\n",
    "        df_tab1 = pd.DataFrame()\n",
    "\n",
    "    # Tab 2: Financial Metrics (Revenue, Fees, Ratios)\n",
    "    if not df_overview.empty:\n",
    "        for c in ['revenue_24h','revenue_7d','revenue_30d','fees_24h','fees_7d','fees_30d','pf_ratio','pr_ratio','market_cap_usd','tvl']:\n",
    "            if c not in df_overview.columns:\n",
    "                df_overview[c] = 0\n",
    "\n",
    "        financial_cols = [\n",
    "            'name', 'category', 'tvl', 'revenue_24h', 'revenue_7d', 'revenue_30d',\n",
    "            'fees_24h', 'fees_7d', 'fees_30d', 'market_cap_usd', 'pf_ratio', 'pr_ratio'\n",
    "        ]\n",
    "        df_tab2 = df_overview[financial_cols].copy()\n",
    "        df_tab2.rename(columns={\n",
    "            'name': 'Protocol',\n",
    "            'category': 'Category',\n",
    "            'tvl': 'TVL_USD',\n",
    "            'revenue_24h': 'Revenue_24h',\n",
    "            'revenue_7d': 'Revenue_7d',\n",
    "            'revenue_30d': 'Revenue_30d',\n",
    "            'fees_24h': 'Fees_24h',\n",
    "            'fees_7d': 'Fees_7d',\n",
    "            'fees_30d': 'Fees_30d',\n",
    "            'market_cap_usd': 'Market_Cap_USD',\n",
    "            'pf_ratio': 'PF_Ratio',\n",
    "            'pr_ratio': 'PR_Ratio'\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Fill NaN values\n",
    "        for col in ['Revenue_24h','Revenue_7d','Revenue_30d','Fees_24h','Fees_7d','Fees_30d','PF_Ratio','PR_Ratio','Market_Cap_USD','TVL_USD']:\n",
    "            df_tab2[col] = df_tab2[col].fillna(0)\n",
    "\n",
    "        # Filter out protocols with no financial data\n",
    "        df_tab2 = df_tab2[\n",
    "            (df_tab2['Revenue_24h'] > 0) |\n",
    "            (df_tab2['Fees_24h'] > 0) |\n",
    "            (df_tab2['Market_Cap_USD'] > 0)\n",
    "        ].reset_index(drop=True)\n",
    "        print(f\"ğŸ“‹ Tab 2 (Financial) dataset: {df_tab2.shape}\")\n",
    "    else:\n",
    "        df_tab2 = pd.DataFrame()\n",
    "\n",
    "    # Tab 3: Token Distribution Analysis\n",
    "    df_tab3 = process_token_holders_data()\n",
    "    if not df_tab3.empty:\n",
    "        print(f\"ğŸ“‹ Tab 3 (Token Distribution) dataset: {df_tab3.shape}\")\n",
    "\n",
    "    return df_tab1, df_tab2, df_tab3, df_overview\n",
    "\n",
    "# Execute data consolidation\n",
    "df_tab1, df_tab2, df_tab3, df_full = create_streamlit_datasets()\n",
    "\n",
    "print(\"\\nğŸ“ˆ Creating Analysis Summaries...\")\n",
    "\n",
    "def create_category_analysis(df):\n",
    "    \"\"\"Create category-level analysis\"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    category_stats = df.groupby('Category').agg({\n",
    "        'TVL_USD': ['count', 'sum', 'mean', 'median'],\n",
    "        'Market_Cap_USD': ['sum', 'mean'],\n",
    "        'TVL_Change_1d': 'mean',\n",
    "        'Price_Change_24h': 'mean'\n",
    "    }).round(2)\n",
    "\n",
    "    # Flatten column names\n",
    "    category_stats.columns = [f\"{col[0]}_{col[1]}\" for col in category_stats.columns]\n",
    "    category_stats = category_stats.reset_index()\n",
    "\n",
    "    # Rename for clarity\n",
    "    category_stats.rename(columns={\n",
    "        'TVL_USD_count': 'Protocol_Count',\n",
    "        'TVL_USD_sum': 'Total_TVL',\n",
    "        'TVL_USD_mean': 'Avg_TVL',\n",
    "        'TVL_USD_median': 'Median_TVL',\n",
    "        'Market_Cap_USD_sum': 'Total_Market_Cap',\n",
    "        'Market_Cap_USD_mean': 'Avg_Market_Cap',\n",
    "        'TVL_Change_1d_mean': 'Avg_TVL_Change_1d',\n",
    "        'Price_Change_24h_mean': 'Avg_Price_Change_24h'\n",
    "    }, inplace=True)\n",
    "\n",
    "    category_stats = category_stats.sort_values('Total_TVL', ascending=False)\n",
    "    print(f\"ğŸ“Š Category analysis: {len(category_stats)} categories\")\n",
    "    return category_stats\n",
    "\n",
    "def create_financial_rankings(df):\n",
    "    \"\"\"Create various financial rankings\"\"\"\n",
    "    if df.empty:\n",
    "        return {}\n",
    "\n",
    "    rankings = {}\n",
    "\n",
    "    # Top protocols by revenue\n",
    "    if 'Revenue_24h' in df.columns:\n",
    "        rankings['top_revenue'] = df[df['Revenue_24h'] > 0].nlargest(20, 'Revenue_24h')[\n",
    "            ['Protocol', 'Category', 'Revenue_24h', 'Revenue_7d', 'Revenue_30d']\n",
    "        ].copy()\n",
    "    else:\n",
    "        rankings['top_revenue'] = pd.DataFrame()\n",
    "\n",
    "    # Top protocols by fees\n",
    "    if 'Fees_24h' in df.columns:\n",
    "        rankings['top_fees'] = df[df['Fees_24h'] > 0].nlargest(20, 'Fees_24h')[\n",
    "            ['Protocol', 'Category', 'Fees_24h', 'Fees_7d', 'Fees_30d']\n",
    "        ].copy()\n",
    "    else:\n",
    "        rankings['top_fees'] = pd.DataFrame()\n",
    "\n",
    "    # Best P/F ratios (lowest = best value)\n",
    "    if 'PF_Ratio' in df.columns:\n",
    "        rankings['best_pf_ratios'] = df[\n",
    "            (df['PF_Ratio'].notna()) & (df['PF_Ratio'] > 0) & (df['PF_Ratio'] < 100)\n",
    "        ].nsmallest(15, 'PF_Ratio')[\n",
    "            ['Protocol', 'Category', 'Market_Cap_USD', 'Fees_24h', 'PF_Ratio']\n",
    "        ].copy()\n",
    "    else:\n",
    "        rankings['best_pf_ratios'] = pd.DataFrame()\n",
    "\n",
    "    # Best P/R ratios (lowest = best value)\n",
    "    if 'PR_Ratio' in df.columns:\n",
    "        rankings['best_pr_ratios'] = df[\n",
    "            (df['PR_Ratio'].notna()) & (df['PR_Ratio'] > 0) & (df['PR_Ratio'] < 100)\n",
    "        ].nsmallest(15, 'PR_Ratio')[\n",
    "            ['Protocol', 'Category', 'Market_Cap_USD', 'Revenue_24h', 'PR_Ratio']\n",
    "        ].copy()\n",
    "    else:\n",
    "        rankings['best_pr_ratios'] = pd.DataFrame()\n",
    "\n",
    "    print(f\"ğŸ“Š Created {len(rankings)} financial ranking tables\")\n",
    "    return rankings\n",
    "\n",
    "def create_token_distribution_summary(df):\n",
    "    \"\"\"Create token distribution analysis summary\"\"\"\n",
    "    if df.empty:\n",
    "        return df, pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Summary statistics\n",
    "    summary_stats = df.agg({\n",
    "        'total_accounts_analyzed': 'mean',\n",
    "        'top_1_holder_share': ['mean', 'median', 'std'],\n",
    "        'top_5_holders_share': ['mean', 'median', 'std'],\n",
    "        'top_10_holders_share': ['mean', 'median', 'std'],\n",
    "        'gini_coefficient': ['mean', 'median', 'std']\n",
    "    }).round(3)\n",
    "\n",
    "    # Create distribution categories\n",
    "    df = df.copy()\n",
    "    df['concentration_level'] = pd.cut(\n",
    "        df['top_10_holders_share'],\n",
    "        bins=[0, 25, 50, 75, 100],\n",
    "        labels=['Low (0-25%)', 'Medium (25-50%)', 'High (50-75%)', 'Very High (75-100%)']\n",
    "    )\n",
    "\n",
    "    concentration_dist = df['concentration_level'].value_counts().reset_index()\n",
    "    concentration_dist.rename(columns={'index': 'Concentration_Level', 'concentration_level': 'Token_Count'}, inplace=True)\n",
    "\n",
    "    print(f\"ğŸ“Š Token distribution summary: {len(df)} tokens analyzed\")\n",
    "    return df, summary_stats, concentration_dist\n",
    "\n",
    "# Execute analysis\n",
    "category_analysis = create_category_analysis(df_tab1)\n",
    "financial_rankings = create_financial_rankings(df_tab2)\n",
    "df_tab3_enhanced, distribution_summary, concentration_dist = create_token_distribution_summary(df_tab3)\n",
    "\n",
    "print(\"\\nğŸ’¾ Saving Streamlit-Ready Datasets...\")\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save datasets for Streamlit\n",
    "datasets = {\n",
    "    'tab1_overview': df_tab1,\n",
    "    'tab2_financial': df_tab2,\n",
    "    'tab3_distribution': df_tab3_enhanced,\n",
    "    'category_analysis': category_analysis,\n",
    "    'financial_rankings': financial_rankings,\n",
    "    'distribution_summary': distribution_summary,\n",
    "    'concentration_distribution': concentration_dist,\n",
    "    'full_dataset': df_full\n",
    "}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    if isinstance(dataset, pd.DataFrame) and not dataset.empty:\n",
    "        filepath = os.path.join(streamlit_data_dir, f'{name}_{timestamp}.joblib')\n",
    "        joblib.dump(dataset, filepath, compress='zlib')\n",
    "        print(f\"ğŸ’¾ Saved {name}: {filepath} (Shape: {dataset.shape})\")\n",
    "    elif isinstance(dataset, dict) and dataset:\n",
    "        filepath = os.path.join(streamlit_data_dir, f'{name}_{timestamp}.joblib')\n",
    "        joblib.dump(dataset, filepath, compress='zlib')\n",
    "        print(f\"ğŸ’¾ Saved {name}: {filepath} (Dict with {len(dataset)} items)\")\n",
    "\n",
    "print(\"\\nğŸ¯ Creating Summary Statistics for Streamlit...\")\n",
    "\n",
    "# Create summary statistics\n",
    "summary_stats = {}\n",
    "\n",
    "if not df_tab1.empty:\n",
    "    summary_stats['overview'] = {\n",
    "        'total_protocols': len(df_tab1),\n",
    "        'total_tvl': df_tab1['TVL_USD'].sum(),\n",
    "        'avg_tvl': df_tab1['TVL_USD'].mean(),\n",
    "        'total_market_cap': df_tab1['Market_Cap_USD'].sum(),\n",
    "        'protocols_with_tokens': len(df_tab1[df_tab1['Market_Cap_USD'] > 0]),\n",
    "        'top_category': df_tab1.groupby('Category')['TVL_USD'].sum().idxmax() if not df_tab1.empty else 'N/A',\n",
    "        'most_valuable_protocol': df_tab1.loc[df_tab1['TVL_USD'].idxmax(), 'Protocol'] if not df_tab1.empty else 'N/A',\n",
    "        'timestamp': datetime.now()\n",
    "    }\n",
    "\n",
    "if not df_tab2.empty:\n",
    "    summary_stats['financial'] = {\n",
    "        'protocols_with_revenue': len(df_tab2[df_tab2['Revenue_24h'] > 0]),\n",
    "        'protocols_with_fees': len(df_tab2[df_tab2['Fees_24h'] > 0]),\n",
    "        'total_daily_revenue': df_tab2['Revenue_24h'].sum(),\n",
    "        'total_daily_fees': df_tab2['Fees_24h'].sum(),\n",
    "        'avg_pf_ratio': df_tab2['PF_Ratio'].median(),\n",
    "        'avg_pr_ratio': df_tab2['PR_Ratio'].median(),\n",
    "        'highest_revenue_protocol': df_tab2.loc[df_tab2['Revenue_24h'].idxmax(), 'Protocol'] if df_tab2['Revenue_24h'].max() > 0 else 'N/A',\n",
    "        'timestamp': datetime.now()\n",
    "    }\n",
    "\n",
    "if not df_tab3.empty:\n",
    "    summary_stats['distribution'] = {\n",
    "        'tokens_analyzed': len(df_tab3),\n",
    "        'avg_gini_coefficient': df_tab3['gini_coefficient'].mean(),\n",
    "        'avg_top_10_share': df_tab3['top_10_holders_share'].mean(),\n",
    "        'most_concentrated_token': df_tab3.loc[df_tab3['top_10_holders_share'].idxmax(), 'token_name'] if not df_tab3.empty else 'N/A',\n",
    "        'least_concentrated_token': df_tab3.loc[df_tab3['top_10_holders_share'].idxmin(), 'token_name'] if not df_tab3.empty else 'N/A',\n",
    "        'timestamp': datetime.now()\n",
    "    }\n",
    "\n",
    "# Save summary statistics\n",
    "summary_filepath = os.path.join(streamlit_data_dir, f'summary_stats_{timestamp}.joblib')\n",
    "joblib.dump(summary_stats, summary_filepath, compress='zlib')\n",
    "print(f\"ğŸ’¾ Saved summary statistics: {summary_filepath}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Data Processing Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Ready for Streamlit Application:\")\n",
    "print(f\"  ğŸ“ Data directory: {streamlit_data_dir}\")\n",
    "print(f\"  ğŸ“Š Tab 1 (Overview): {df_tab1.shape if not df_tab1.empty else 'No data'}\")\n",
    "print(f\"  ğŸ’° Tab 2 (Financial): {df_tab2.shape if not df_tab2.empty else 'No data'}\")\n",
    "print(f\"  ğŸ¯ Tab 3 (Distribution): {df_tab3.shape if not df_tab3.empty else 'No data'}\")\n",
    "\n",
    "if summary_stats:\n",
    "    print(\"\\nğŸ“ˆ Key Metrics Summary:\")\n",
    "    if 'overview' in summary_stats:\n",
    "        print(f\"  â€¢ Total Protocols: {summary_stats['overview']['total_protocols']}\")\n",
    "        print(f\"  â€¢ Total TVL: {format_currency(summary_stats['overview']['total_tvl'])}\")\n",
    "        print(f\"  â€¢ Total Market Cap: {format_currency(summary_stats['overview']['total_market_cap'])}\")\n",
    "\n",
    "    if 'financial' in summary_stats:\n",
    "        print(f\"  â€¢ Daily Revenue: {format_currency(summary_stats['financial']['total_daily_revenue'])}\")\n",
    "        print(f\"  â€¢ Daily Fees: {format_currency(summary_stats['financial']['total_daily_fees'])}\")\n",
    "\n",
    "    if 'distribution' in summary_stats:\n",
    "        print(f\"  â€¢ Tokens Analyzed: {summary_stats['distribution']['tokens_analyzed']}\")\n",
    "        print(f\"  â€¢ Avg Gini Coefficient: {summary_stats['distribution']['avg_gini_coefficient']:.3f}\")\n",
    "\n",
    "print(\"\\nğŸš€ Data is ready for Streamlit dashboard implementation!\")\n",
    "\n",
    "# Create a metadata file for easy data loading in Streamlit\n",
    "metadata = {\n",
    "    'last_updated': datetime.now(),\n",
    "    'data_files': {\n",
    "        'tab1_overview': f'tab1_overview_{timestamp}.joblib',\n",
    "        'tab2_financial': f'tab2_financial_{timestamp}.joblib',\n",
    "        'tab3_distribution': f'tab3_distribution_{timestamp}.joblib',\n",
    "        'category_analysis': f'category_analysis_{timestamp}.joblib',\n",
    "        'financial_rankings': f'financial_rankings_{timestamp}.joblib',\n",
    "        'summary_stats': f'summary_stats_{timestamp}.joblib'\n",
    "    },\n",
    "    'record_counts': {\n",
    "        'protocols_overview': len(df_tab1),\n",
    "        'protocols_financial': len(df_tab2),\n",
    "        'tokens_distribution': len(df_tab3)\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_filepath = os.path.join(streamlit_data_dir, 'latest_data_metadata.joblib')\n",
    "joblib.dump(metadata, metadata_filepath)\n",
    "print(f\"ğŸ’¾ Saved metadata: {metadata_filepath}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6209396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
