{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a837cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Solana DeFi Tracker - Analysis for Streamlit\n",
      "Analysis timestamp: 2025-09-02 00:40:08\n",
      "\n",
      "ðŸ” Loading Collected Data...\n",
      "âœ… Loaded TVL: (250, 14)\n",
      "âœ… Loaded Revenue: (117, 7)\n",
      "âœ… Loaded Fees: (117, 7)\n",
      "âœ… Loaded CoinGecko: (80, 20)\n",
      "âœ… Loaded Holders: (616, 10)\n",
      "ðŸ’¾ Saved raw revenue: (117, 8)\n",
      "ðŸ’¾ Saved raw fees: (117, 8)\n",
      "\n",
      "ðŸ”„ Processing Data...\n",
      "\n",
      "ðŸ“Š Processing 250 protocols for TVL data\n",
      "âœ… CoinGecko for TVL: 80/250 protocols matched\n",
      "ðŸ“‹ Tab 1 (Overview): (250, 8)\n",
      "ðŸ“‹ Tab 2 (Revenue): (117, 4)\n",
      "ðŸ“‹ Tab 2 (Fees): (117, 4)\n",
      "\n",
      "ðŸ“Š Starting metrics with 117 protocols from revenue data\n",
      "âœ… Fees matched: 117/117\n",
      "âœ… CoinGecko for metrics: 19/117 protocols matched\n",
      "ðŸ“Š Calculated ratios: P/F=14, P/R=14\n",
      "ðŸ“‹ Tab 3 (Financial Metrics): (117, 10)\n",
      "âœ… Matched prices for 60/67 tokens\n",
      "âœ… Processed 67 tokens for concentration analysis\n",
      "ðŸ“‹ Tab 4 (Distribution): (67, 10)\n",
      "ðŸ’¾ Prepared raw token holders: 67 tokens\n",
      "\n",
      "ðŸ’¾ Saving Streamlit Datasets...\n",
      "ðŸ’¾ Saved tab1_overview: (250, 8)\n",
      "ðŸ’¾ Saved tab2_revenue: (117, 4)\n",
      "ðŸ’¾ Saved tab2_fees: (117, 4)\n",
      "ðŸ’¾ Saved tab3_metrics: (117, 10)\n",
      "ðŸ’¾ Saved tab4_distribution: (67, 10)\n",
      "ðŸ’¾ Saved category_analysis: (39, 9)\n",
      "ðŸ’¾ Saved financial_rankings: 4 items\n",
      "ðŸ’¾ Saved summary_stats: 3 items\n",
      "ðŸ’¾ Saved raw_token_holders: 67 items\n",
      "ðŸ’¾ Saved metadata: latest_data_metadata.joblib\n",
      "\n",
      "âœ… Analysis Complete!\n",
      "ðŸ“Š Overview: 250 protocols\n",
      "ðŸ’µ Revenue: 117 protocols\n",
      "ðŸ’¸ Fees: 117 protocols\n",
      "ðŸ“ˆ Metrics: 117 protocols\n",
      "ðŸŽ¯ Distribution: 67 tokens\n",
      "\n",
      "ðŸ“ˆ Financial Summary:\n",
      " Daily Revenue: $15.62M\n",
      " Daily Fees: $15.62M\n",
      " Avg P/F Ratio: 10.418896250141671\n",
      " Avg P/R Ratio: 10.418896250141671\n",
      "ðŸš€ Ready for Streamlit!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Set up paths\n",
    "parent_dir = str(Path().resolve().parent)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Set up directories\n",
    "processed_dir = os.path.normpath('../data/processed')\n",
    "api_responses_dir = os.path.normpath('../data/api_responses')\n",
    "streamlit_data_dir = os.path.normpath('../data/streamlit')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "os.makedirs(streamlit_data_dir, exist_ok=True)\n",
    "\n",
    "print(\"ðŸ“Š Solana DeFi Tracker - Analysis for Streamlit\")\n",
    "print(f\"Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "def load_latest_file(directory, pattern):\n",
    "    \"\"\"Load the most recent file matching the pattern\"\"\"\n",
    "    files = glob.glob(os.path.join(directory, pattern))\n",
    "    if not files:\n",
    "        return None, None\n",
    "    latest_file = max(files, key=os.path.getctime)\n",
    "    return joblib.load(latest_file), latest_file\n",
    "\n",
    "def format_currency(amount):\n",
    "    \"\"\"Format currency amount with appropriate units\"\"\"\n",
    "    if amount is None or amount == 0:\n",
    "        return \"$0\"\n",
    "    if amount >= 1_000_000_000:\n",
    "        return f\"${amount/1_000_000_000:.2f}B\"\n",
    "    elif amount >= 1_000_000:\n",
    "        return f\"${amount/1_000_000:.2f}M\"\n",
    "    elif amount >= 1_000:\n",
    "        return f\"${amount/1_000:.2f}K\"\n",
    "    else:\n",
    "        return f\"${amount:.2f}\"\n",
    "\n",
    "print(\"\\nðŸ” Loading Collected Data...\")\n",
    "# Load all datasets\n",
    "tvl_data, _ = load_latest_file(api_responses_dir, 'solana_defi_tvl_*.joblib')\n",
    "revenue_data, _ = load_latest_file(api_responses_dir, 'solana_revenue_*.joblib')\n",
    "fees_data, _ = load_latest_file(api_responses_dir, 'solana_fees_*.joblib')\n",
    "coingecko_data, _ = load_latest_file(api_responses_dir, 'solana_coingecko_enhanced_*.joblib')\n",
    "holders_data, _ = load_latest_file(api_responses_dir, 'solana_token_holders_*.joblib')\n",
    "\n",
    "# Convert fees_data to DataFrame if it's a list\n",
    "if isinstance(fees_data, list):\n",
    "    fees_data = pd.DataFrame(fees_data)\n",
    "\n",
    "# Convert CoinGecko data to DataFrame\n",
    "coingecko_df = pd.DataFrame()\n",
    "if isinstance(coingecko_data, dict):\n",
    "    coingecko_df = pd.DataFrame.from_dict(coingecko_data, orient='index').reset_index()\n",
    "    coingecko_df.rename(columns={'index': 'protocol_key'}, inplace=True)\n",
    "elif isinstance(coingecko_data, (list, pd.DataFrame)):\n",
    "    coingecko_df = pd.DataFrame(coingecko_data) if isinstance(coingecko_data, list) else coingecko_data\n",
    "\n",
    "print(f\"âœ… Loaded TVL: {tvl_data.shape if isinstance(tvl_data, pd.DataFrame) else 'Not available'}\")\n",
    "print(f\"âœ… Loaded Revenue: {revenue_data.shape if isinstance(revenue_data, pd.DataFrame) else 'Not available'}\")\n",
    "print(f\"âœ… Loaded Fees: {fees_data.shape if isinstance(fees_data, pd.DataFrame) else 'Not available'}\")\n",
    "print(f\"âœ… Loaded CoinGecko: {coingecko_df.shape if not coingecko_df.empty else 'Not available'}\")\n",
    "print(f\"âœ… Loaded Holders: {holders_data.shape if isinstance(holders_data, pd.DataFrame) else 'Not available'}\")\n",
    "\n",
    "# Save raw revenue and fees as DataFrames\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "if isinstance(revenue_data, pd.DataFrame) and not revenue_data.empty:\n",
    "    revenue_clean = revenue_data.copy()\n",
    "    revenue_clean['protocol_clean'] = revenue_clean['protocol'].astype(str).str.lower().str.strip()\n",
    "    revenue_clean = revenue_clean.rename(columns={\n",
    "        'total_24h': 'Revenue_24h',\n",
    "        'total_7d': 'Revenue_7d',\n",
    "        'total_30d': 'Revenue_30d'\n",
    "    })\n",
    "    revenue_filepath = os.path.join(streamlit_data_dir, f'raw_revenue_{timestamp}.joblib')\n",
    "    joblib.dump(revenue_clean, revenue_filepath, compress='zlib')\n",
    "    print(f\"ðŸ’¾ Saved raw revenue: {revenue_clean.shape}\")\n",
    "\n",
    "if isinstance(fees_data, pd.DataFrame) and not fees_data.empty:\n",
    "    fees_clean = fees_data.copy()\n",
    "    fees_clean['protocol_clean'] = fees_clean['protocol'].astype(str).str.lower().str.strip()\n",
    "    fees_clean = fees_clean.rename(columns={\n",
    "        'total_24h': 'Fees_24h',\n",
    "        'total_7d': 'Fees_7d',\n",
    "        'total_30d': 'Fees_30d'\n",
    "    })\n",
    "    fees_filepath = os.path.join(streamlit_data_dir, f'raw_fees_{timestamp}.joblib')\n",
    "    joblib.dump(fees_clean, fees_filepath, compress='zlib')\n",
    "    print(f\"ðŸ’¾ Saved raw fees: {fees_clean.shape}\")\n",
    "\n",
    "def process_tvl_data():\n",
    "    \"\"\"Process TVL data for Overview tab\"\"\"\n",
    "    if not isinstance(tvl_data, pd.DataFrame) or tvl_data.empty:\n",
    "        print(\"âŒ No TVL data available\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df_base = tvl_data.copy()\n",
    "    df_base['protocol_clean'] = df_base['name'].astype(str).str.lower().str.strip()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Processing {len(df_base)} protocols for TVL data\")\n",
    "    \n",
    "    # Merge CoinGecko data\n",
    "    if not coingecko_df.empty:\n",
    "        cg = coingecko_df.copy()\n",
    "        if 'protocol_name' in cg.columns:\n",
    "            cg['protocol_clean'] = cg['protocol_name'].astype(str).str.lower().str.strip()\n",
    "        elif 'protocol_key' in cg.columns:\n",
    "            cg['protocol_clean'] = cg['protocol_key'].astype(str).str.lower().str.strip()\n",
    "        \n",
    "        cg_cols = ['protocol_clean', 'symbol', 'current_price_usd', 'market_cap_usd',\n",
    "                   'price_change_24h_percent', 'price_change_7d_percent', 'price_change_30d_percent']\n",
    "        cg_cols = [c for c in cg_cols if c in cg.columns]\n",
    "        \n",
    "        if 'protocol_clean' in cg_cols:\n",
    "            cg_merge = cg[cg_cols].dropna(subset=['protocol_clean']).drop_duplicates('protocol_clean')\n",
    "            df_base = df_base.merge(cg_merge, on='protocol_clean', how='left')\n",
    "            matched_cg = df_base['market_cap_usd'].notna().sum()\n",
    "            print(f\"âœ… CoinGecko for TVL: {matched_cg}/{len(df_base)} protocols matched\")\n",
    "    \n",
    "    # Convert to numeric and fill NaN\n",
    "    numeric_cols = ['market_cap_usd', 'tvl']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_base.columns:\n",
    "            df_base[col] = pd.to_numeric(df_base[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Calculate MCAP/TVL ratio\n",
    "    df_base['mcap_tvl_ratio'] = np.where(\n",
    "        df_base['tvl'] > 0,\n",
    "        df_base['market_cap_usd'] / df_base['tvl'],\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    return df_base\n",
    "\n",
    "def process_financial_metrics():\n",
    "    \"\"\"Process revenue, fees, and CoinGecko for financial metrics (P/F, P/R ratios)\"\"\"\n",
    "    df_metrics = pd.DataFrame()\n",
    "    \n",
    "    # Start with revenue if available\n",
    "    if isinstance(revenue_data, pd.DataFrame) and not revenue_data.empty:\n",
    "        df_metrics = revenue_data.copy()\n",
    "        df_metrics['protocol_clean'] = df_metrics['protocol'].astype(str).str.lower().str.strip()\n",
    "        df_metrics = df_metrics.rename(columns={\n",
    "            'total_24h': 'Revenue_24h',\n",
    "            'total_7d': 'Revenue_7d',\n",
    "            'total_30d': 'Revenue_30d'\n",
    "        })\n",
    "        print(f\"\\nðŸ“Š Starting metrics with {len(df_metrics)} protocols from revenue data\")\n",
    "    \n",
    "    # Add fees if available\n",
    "    if isinstance(fees_data, pd.DataFrame) and not fees_data.empty:\n",
    "        fees_clean = fees_data.copy()\n",
    "        fees_clean['protocol_clean'] = fees_clean['protocol'].astype(str).str.lower().str.strip()\n",
    "        fees_clean = fees_clean.rename(columns={\n",
    "            'total_24h': 'Fees_24h',\n",
    "            'total_7d': 'Fees_7d',\n",
    "            'total_30d': 'Fees_30d'\n",
    "        })\n",
    "        fees_merge = fees_clean[['protocol_clean', 'Fees_24h', 'Fees_7d', 'Fees_30d']].drop_duplicates('protocol_clean')\n",
    "        if df_metrics.empty:\n",
    "            df_metrics = fees_merge.copy()\n",
    "            print(f\"\\nðŸ“Š Starting metrics with {len(df_metrics)} protocols from fees data\")\n",
    "        else:\n",
    "            df_metrics = df_metrics.merge(fees_merge, on='protocol_clean', how='outer')\n",
    "        matched_fees = df_metrics['Fees_24h'].notna().sum()\n",
    "        print(f\"âœ… Fees matched: {matched_fees}/{len(df_metrics)}\")\n",
    "    \n",
    "    # Merge CoinGecko for market cap\n",
    "    if not coingecko_df.empty and not df_metrics.empty:\n",
    "        cg = coingecko_df.copy()\n",
    "        if 'protocol_name' in cg.columns:\n",
    "            cg['protocol_clean'] = cg['protocol_name'].astype(str).str.lower().str.strip()\n",
    "        elif 'protocol_key' in cg.columns:\n",
    "            cg['protocol_clean'] = cg['protocol_key'].astype(str).str.lower().str.strip()\n",
    "        \n",
    "        cg_cols = ['protocol_clean', 'market_cap_usd']\n",
    "        cg_cols = [c for c in cg_cols if c in cg.columns]\n",
    "        \n",
    "        if 'protocol_clean' in cg_cols:\n",
    "            cg_merge = cg[cg_cols].dropna(subset=['protocol_clean']).drop_duplicates('protocol_clean')\n",
    "            df_metrics = df_metrics.merge(cg_merge, on='protocol_clean', how='left')\n",
    "            matched_cg = df_metrics['market_cap_usd'].notna().sum()\n",
    "            print(f\"âœ… CoinGecko for metrics: {matched_cg}/{len(df_metrics)} protocols matched\")\n",
    "    \n",
    "    # Convert to numeric\n",
    "    numeric_cols = ['Revenue_24h', 'Revenue_7d', 'Revenue_30d', 'Fees_24h', 'Fees_7d', 'Fees_30d', 'market_cap_usd']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_metrics.columns:\n",
    "            df_metrics[col] = pd.to_numeric(df_metrics[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Calculate ratios\n",
    "    df_metrics['PF_Ratio'] = np.where(\n",
    "        (df_metrics['Fees_24h'] > 0) & (df_metrics['market_cap_usd'] > 0),\n",
    "        df_metrics['market_cap_usd'] / (df_metrics['Fees_24h'] * 365),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    df_metrics['PR_Ratio'] = np.where(\n",
    "        (df_metrics['Revenue_24h'] > 0) & (df_metrics['market_cap_usd'] > 0),\n",
    "        df_metrics['market_cap_usd'] / (df_metrics['Revenue_24h'] * 365),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Debug ratios\n",
    "    valid_pf = df_metrics['PF_Ratio'].notna().sum()\n",
    "    valid_pr = df_metrics['PR_Ratio'].notna().sum()\n",
    "    print(f\"ðŸ“Š Calculated ratios: P/F={valid_pf}, P/R={valid_pr}\")\n",
    "    \n",
    "    return df_metrics\n",
    "\n",
    "def process_token_holders_data():\n",
    "    \"\"\"Process token holders data for concentration analysis, add price from CoinGecko\"\"\"\n",
    "    if not isinstance(holders_data, pd.DataFrame) or holders_data.empty:\n",
    "        print(\"âŒ No token holders data available\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    token_df_sorted = holders_data.sort_values(['token_name', 'rank']).copy()\n",
    "    concentration_metrics = []\n",
    "    \n",
    "    for token_name in token_df_sorted['token_name'].unique():\n",
    "        token_df = token_df_sorted[token_df_sorted['token_name'] == token_name]\n",
    "        if len(token_df) > 0:\n",
    "            total_supply = token_df['ui_amount'].sum()\n",
    "            top_1_share = (token_df.iloc[0]['ui_amount'] / total_supply * 100) if total_supply > 0 else 0\n",
    "            top_5_share = (token_df.head(5)['ui_amount'].sum() / total_supply * 100) if total_supply > 0 else 0\n",
    "            top_10_share = (token_df.head(10)['ui_amount'].sum() / total_supply * 100) if total_supply > 0 else 0\n",
    "            # Gini coefficient\n",
    "            balances = token_df['ui_amount'].values\n",
    "            balances = balances[balances > 0]\n",
    "            if len(balances) > 1:\n",
    "                n = len(balances)\n",
    "                mean_balance = np.mean(balances)\n",
    "                if mean_balance > 0:\n",
    "                    diffs = np.abs(balances.reshape(-1, 1) - balances.reshape(1, -1))\n",
    "                    gini = diffs.sum() / (2 * n * n * mean_balance)\n",
    "                else:\n",
    "                    gini = 0\n",
    "            else:\n",
    "                gini = 0\n",
    "            concentration_metrics.append({\n",
    "                'token_name': token_name,\n",
    "                'token_symbol': token_df.iloc[0].get('token_symbol', ''),\n",
    "                'total_accounts_analyzed': len(token_df),\n",
    "                'top_1_holder_share': top_1_share,\n",
    "                'top_5_holders_share': top_5_share,\n",
    "                'top_10_holders_share': top_10_share,\n",
    "                'gini_coefficient': gini,\n",
    "                'largest_holder_amount': token_df.iloc[0]['ui_amount']\n",
    "            })\n",
    "    \n",
    "    concentration_df = pd.DataFrame(concentration_metrics)\n",
    "    \n",
    "    # Add token price from coingecko_df\n",
    "    if not coingecko_df.empty and 'symbol' in coingecko_df.columns and 'current_price_usd' in coingecko_df.columns:\n",
    "        coingecko_df['token_clean'] = coingecko_df['symbol'].astype(str).str.lower().str.strip()\n",
    "        concentration_df['token_clean'] = concentration_df['token_symbol'].astype(str).str.lower().str.strip()\n",
    "        \n",
    "        price_merge = coingecko_df[['token_clean', 'current_price_usd']].drop_duplicates('token_clean')\n",
    "        concentration_df = concentration_df.merge(price_merge, on='token_clean', how='left')\n",
    "        concentration_df.rename(columns={'current_price_usd': 'token_price_usd'}, inplace=True)\n",
    "        concentration_df['token_price_usd'] = pd.to_numeric(concentration_df['token_price_usd'], errors='coerce').fillna(0)\n",
    "        \n",
    "        matched_prices = concentration_df['token_price_usd'].gt(0).sum()\n",
    "        print(f\"âœ… Matched prices for {matched_prices}/{len(concentration_df)} tokens\")\n",
    "    \n",
    "    print(f\"âœ… Processed {len(concentration_df)} tokens for concentration analysis\")\n",
    "    return concentration_df\n",
    "\n",
    "def process_raw_holders_data():\n",
    "    \"\"\"Process raw token holders data using 'account_address' column\"\"\"\n",
    "    if not isinstance(holders_data, pd.DataFrame) or holders_data.empty:\n",
    "        print(\"âŒ No token holders data available\")\n",
    "        return {}\n",
    "\n",
    "    if 'account_address' not in holders_data.columns:\n",
    "        print(\"âš ï¸ Warning: No 'account_address' column found, using index as placeholder\")\n",
    "        holders_data['account_address'] = holders_data.index.astype(str)\n",
    "\n",
    "    raw_holders = {}\n",
    "    for token_name in holders_data['token_name'].unique():\n",
    "        token_holders = holders_data[holders_data['token_name'] == token_name].copy()\n",
    "        if not token_holders.empty:\n",
    "            required_cols = ['rank', 'account_address', 'ui_amount']\n",
    "            for col in required_cols:\n",
    "                if col not in token_holders.columns:\n",
    "                    token_holders[col] = token_holders.index.astype(str) if col == 'account_address' else 0\n",
    "\n",
    "            raw_holders[token_name] = token_holders[required_cols]\n",
    "\n",
    "    print(f\"ðŸ’¾ Prepared raw token holders: {len(raw_holders)} tokens\")\n",
    "    return raw_holders\n",
    "\n",
    "\n",
    "def create_streamlit_datasets():\n",
    "    \"\"\"Create datasets for Streamlit tabs\"\"\"\n",
    "    # Tab 1: Overview (TVL-based)\n",
    "    df_tvl = process_tvl_data()\n",
    "    if not df_tvl.empty:\n",
    "        overview_cols = ['name', 'category', 'tvl', 'market_cap_usd', 'current_price_usd',\n",
    "                        'change_1d', 'price_change_24h_percent', 'mcap_tvl_ratio']\n",
    "        \n",
    "        for col in overview_cols:\n",
    "            if col not in df_tvl.columns:\n",
    "                df_tvl[col] = np.nan if col not in ['name', 'category'] else ''\n",
    "        df_tab1 = df_tvl[overview_cols].copy()\n",
    "        df_tab1.rename(columns={\n",
    "            'name': 'Protocol', 'category': 'Category', 'tvl': 'TVL_USD',\n",
    "            'market_cap_usd': 'Market_Cap_USD', 'current_price_usd': 'Price_USD',\n",
    "            'change_1d': 'TVL_Change_1d', 'price_change_24h_percent': 'Price_Change_24h',\n",
    "            'mcap_tvl_ratio': 'MCap_TVL_Ratio'\n",
    "        }, inplace=True)\n",
    "        numeric_cols = ['Market_Cap_USD', 'Price_USD', 'TVL_Change_1d', 'Price_Change_24h']\n",
    "        for col in numeric_cols:\n",
    "            if col in df_tab1.columns:\n",
    "                df_tab1[col] = pd.to_numeric(df_tab1[col], errors='coerce').fillna(0)\n",
    "        df_tab1 = df_tab1.sort_values('TVL_USD', ascending=False).reset_index(drop=True)\n",
    "        print(f\"ðŸ“‹ Tab 1 (Overview): {df_tab1.shape}\")\n",
    "    else:\n",
    "        df_tab1 = pd.DataFrame()\n",
    "    \n",
    "    # Tab 2: Revenue (already saved as raw_revenue)\n",
    "    df_revenue = revenue_clean if isinstance(revenue_data, pd.DataFrame) and not revenue_data.empty else pd.DataFrame()\n",
    "    if not df_revenue.empty:\n",
    "        df_revenue = df_revenue[['protocol_clean', 'Revenue_24h', 'Revenue_7d', 'Revenue_30d']].rename(columns={'protocol_clean': 'Protocol'})\n",
    "        df_revenue = df_revenue.sort_values('Revenue_24h', ascending=False).reset_index(drop=True)\n",
    "        print(f\"ðŸ“‹ Tab 2 (Revenue): {df_revenue.shape}\")\n",
    "    \n",
    "    # Tab 2: Fees (already saved as raw_fees)\n",
    "    df_fees = fees_clean if isinstance(fees_data, pd.DataFrame) and not fees_data.empty else pd.DataFrame()\n",
    "    if not df_fees.empty:\n",
    "        df_fees = df_fees[['protocol_clean', 'Fees_24h', 'Fees_7d', 'Fees_30d']].rename(columns={'protocol_clean': 'Protocol'})\n",
    "        df_fees = df_fees.sort_values('Fees_24h', ascending=False).reset_index(drop=True)\n",
    "        print(f\"ðŸ“‹ Tab 2 (Fees): {df_fees.shape}\")\n",
    "    \n",
    "    # Tab 3: Financial Metrics\n",
    "    df_tab3 = process_financial_metrics()\n",
    "    if not df_tab3.empty:\n",
    "        metrics_cols = ['protocol_clean', 'Revenue_24h', 'Fees_24h', 'Revenue_7d', 'Fees_7d', 'Revenue_30d', 'Fees_30d', 'market_cap_usd', 'PF_Ratio', 'PR_Ratio']\n",
    "        df_tab3 = df_tab3[metrics_cols].rename(columns={'protocol_clean': 'Protocol', 'market_cap_usd': 'Market_Cap_USD'})\n",
    "        df_tab3 = df_tab3.sort_values('Revenue_24h', ascending=False).reset_index(drop=True)\n",
    "        print(f\"ðŸ“‹ Tab 3 (Financial Metrics): {df_tab3.shape}\")\n",
    "    \n",
    "    # Tab 4: Token Distribution\n",
    "    df_tab4 = process_token_holders_data()\n",
    "    if not df_tab4.empty:\n",
    "        df_tab4 = df_tab4.sort_values('gini_coefficient', ascending=False).reset_index(drop=True)\n",
    "        print(f\"ðŸ“‹ Tab 4 (Distribution): {df_tab4.shape}\")\n",
    "    \n",
    "    # Raw token holders\n",
    "    raw_holders = process_raw_holders_data()\n",
    "    \n",
    "    return df_tab1, df_revenue, df_fees, df_tab3, df_tab4, raw_holders\n",
    "\n",
    "def create_category_analysis(df):\n",
    "    \"\"\"Create category-level analysis for Tab 1\"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    category_stats = df.groupby('Category').agg({\n",
    "        'TVL_USD': ['count', 'sum', 'mean', 'median'],\n",
    "        'Market_Cap_USD': ['sum', 'mean'],\n",
    "        'TVL_Change_1d': 'mean',\n",
    "        'Price_Change_24h': 'mean'\n",
    "    }).round(2)\n",
    "    category_stats.columns = [f\"{col[0]}_{col[1]}\" for col in category_stats.columns]\n",
    "    category_stats = category_stats.reset_index()\n",
    "    category_stats.rename(columns={\n",
    "        'TVL_USD_count': 'Protocol_Count',\n",
    "        'TVL_USD_sum': 'Total_TVL',\n",
    "        'TVL_USD_mean': 'Avg_TVL',\n",
    "        'TVL_USD_median': 'Median_TVL',\n",
    "        'Market_Cap_USD_sum': 'Total_Market_Cap',\n",
    "        'Market_Cap_USD_mean': 'Avg_Market_Cap',\n",
    "        'TVL_Change_1d_mean': 'Avg_TVL_Change_1d',\n",
    "        'Price_Change_24h_mean': 'Avg_Price_Change_24h'\n",
    "    }, inplace=True)\n",
    "    return category_stats.sort_values('Total_TVL', ascending=False).reset_index(drop=True)\n",
    "\n",
    "def create_financial_rankings(df):\n",
    "    \"\"\"Create financial rankings for Tab 3\"\"\"\n",
    "    if df.empty:\n",
    "        return {}\n",
    "    rankings = {}\n",
    "    if 'Revenue_24h' in df.columns:\n",
    "        rankings['top_revenue'] = df[df['Revenue_24h'] > 0].nlargest(20, 'Revenue_24h')[\n",
    "            ['Protocol', 'Revenue_24h', 'Revenue_7d', 'Revenue_30d']\n",
    "        ].reset_index(drop=True)\n",
    "    if 'Fees_24h' in df.columns:\n",
    "        rankings['top_fees'] = df[df['Fees_24h'] > 0].nlargest(20, 'Fees_24h')[\n",
    "            ['Protocol', 'Fees_24h', 'Fees_7d', 'Fees_30d']\n",
    "        ].reset_index(drop=True)\n",
    "    if 'PF_Ratio' in df.columns:\n",
    "        valid_pf = df[(df['PF_Ratio'].notna()) & (df['PF_Ratio'] > 0) & (df['PF_Ratio'] < 100)]\n",
    "        if not valid_pf.empty:\n",
    "            rankings['best_pf_ratios'] = valid_pf.nsmallest(15, 'PF_Ratio')[\n",
    "                ['Protocol', 'Market_Cap_USD', 'Fees_24h', 'PF_Ratio']\n",
    "            ].reset_index(drop=True)\n",
    "    if 'PR_Ratio' in df.columns:\n",
    "        valid_pr = df[(df['PR_Ratio'].notna()) & (df['PR_Ratio'] > 0) & (df['PR_Ratio'] < 100)]\n",
    "        if not valid_pr.empty:\n",
    "            rankings['best_pr_ratios'] = valid_pr.nsmallest(15, 'PR_Ratio')[\n",
    "                ['Protocol', 'Market_Cap_USD', 'Revenue_24h', 'PR_Ratio']\n",
    "            ].reset_index(drop=True)\n",
    "    return rankings\n",
    "\n",
    "def create_summary_stats(df_tab1, df_revenue, df_fees, df_tab3, df_tab4):\n",
    "    \"\"\"Create summary statistics\"\"\"\n",
    "    summary_stats = {}\n",
    "    if not df_tab1.empty:\n",
    "        summary_stats['overview'] = {\n",
    "            'total_protocols': len(df_tab1),\n",
    "            'total_tvl': df_tab1['TVL_USD'].sum(),\n",
    "            'total_market_cap': df_tab1['Market_Cap_USD'].sum(),\n",
    "            'top_category': df_tab1.groupby('Category')['TVL_USD'].sum().idxmax() if not df_tab1.empty else 'N/A',\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "    if not df_revenue.empty or not df_fees.empty:\n",
    "        summary_stats['financial'] = {\n",
    "            'protocols_with_revenue': len(df_revenue[df_revenue['Revenue_24h'] > 0]) if not df_revenue.empty else 0,\n",
    "            'protocols_with_fees': len(df_fees[df_fees['Fees_24h'] > 0]) if not df_fees.empty else 0,\n",
    "            'total_daily_revenue': df_revenue['Revenue_24h'].sum() if not df_revenue.empty else 0,\n",
    "            'total_daily_fees': df_fees['Fees_24h'].sum() if not df_fees.empty else 0,\n",
    "            'avg_pf_ratio': df_tab3['PF_Ratio'].median() if not df_tab3.empty and 'PF_Ratio' in df_tab3.columns else None,\n",
    "            'avg_pr_ratio': df_tab3['PR_Ratio'].median() if not df_tab3.empty and 'PR_Ratio' in df_tab3.columns else None,\n",
    "            'highest_revenue_protocol': df_revenue.loc[df_revenue['Revenue_24h'].idxmax(), 'Protocol'] if not df_revenue.empty and df_revenue['Revenue_24h'].max() > 0 else 'N/A',\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "    if not df_tab4.empty:\n",
    "        summary_stats['distribution'] = {\n",
    "            'tokens_analyzed': len(df_tab4),\n",
    "            'avg_gini_coefficient': df_tab4['gini_coefficient'].mean(),\n",
    "            'avg_top_10_share': df_tab4['top_10_holders_share'].mean(),\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "    return summary_stats\n",
    "\n",
    "print(\"\\nðŸ”„ Processing Data...\")\n",
    "# Execute data processing\n",
    "df_tab1, df_revenue, df_fees, df_tab3, df_tab4, raw_holders = create_streamlit_datasets()\n",
    "category_analysis = create_category_analysis(df_tab1)\n",
    "financial_rankings = create_financial_rankings(df_tab3)\n",
    "summary_stats = create_summary_stats(df_tab1, df_revenue, df_fees, df_tab3, df_tab4)\n",
    "\n",
    "print(\"\\nðŸ’¾ Saving Streamlit Datasets...\")\n",
    "datasets = {\n",
    "    'tab1_overview': df_tab1,\n",
    "    'tab2_revenue': df_revenue,\n",
    "    'tab2_fees': df_fees,\n",
    "    'tab3_metrics': df_tab3,\n",
    "    'tab4_distribution': df_tab4,\n",
    "    'category_analysis': category_analysis,\n",
    "    'financial_rankings': financial_rankings,\n",
    "    'summary_stats': summary_stats,\n",
    "    'raw_token_holders': raw_holders\n",
    "}\n",
    "for name, dataset in datasets.items():\n",
    "    if isinstance(dataset, pd.DataFrame) and not dataset.empty:\n",
    "        filepath = os.path.join(streamlit_data_dir, f'{name}_{timestamp}.joblib')\n",
    "        joblib.dump(dataset, filepath, compress='zlib')\n",
    "        print(f\"ðŸ’¾ Saved {name}: {dataset.shape}\")\n",
    "    elif isinstance(dataset, dict) and dataset:\n",
    "        filepath = os.path.join(streamlit_data_dir, f'{name}_{timestamp}.joblib')\n",
    "        joblib.dump(dataset, filepath, compress='zlib')\n",
    "        print(f\"ðŸ’¾ Saved {name}: {len(dataset)} items\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'last_updated': datetime.now(),\n",
    "    'data_files': {\n",
    "        'tab1_overview': f'tab1_overview_{timestamp}.joblib',\n",
    "        'tab2_revenue': f'tab2_revenue_{timestamp}.joblib',\n",
    "        'tab2_fees': f'tab2_fees_{timestamp}.joblib',\n",
    "        'tab3_metrics': f'tab3_metrics_{timestamp}.joblib',\n",
    "        'tab4_distribution': f'tab4_distribution_{timestamp}.joblib',\n",
    "        'category_analysis': f'category_analysis_{timestamp}.joblib',\n",
    "        'financial_rankings': f'financial_rankings_{timestamp}.joblib',\n",
    "        'summary_stats': f'summary_stats_{timestamp}.joblib',\n",
    "        'raw_token_holders': f'raw_token_holders_{timestamp}.joblib',\n",
    "        'raw_revenue': f'raw_revenue_{timestamp}.joblib',\n",
    "        'raw_fees': f'raw_fees_{timestamp}.joblib'\n",
    "    },\n",
    "    'record_counts': {\n",
    "        'protocols_overview': len(df_tab1),\n",
    "        'protocols_revenue': len(df_revenue),\n",
    "        'protocols_fees': len(df_fees),\n",
    "        'protocols_metrics': len(df_tab3),\n",
    "        'tokens_distribution': len(df_tab4),\n",
    "        'raw_token_holders': len(raw_holders)\n",
    "    }\n",
    "}\n",
    "metadata_filepath = os.path.join(streamlit_data_dir, 'latest_data_metadata.joblib')\n",
    "joblib.dump(metadata, metadata_filepath)\n",
    "print(f\"ðŸ’¾ Saved metadata: latest_data_metadata.joblib\")\n",
    "\n",
    "print(f\"\\nâœ… Analysis Complete!\")\n",
    "print(f\"ðŸ“Š Overview: {len(df_tab1)} protocols\")\n",
    "print(f\"ðŸ’µ Revenue: {len(df_revenue)} protocols\")\n",
    "print(f\"ðŸ’¸ Fees: {len(df_fees)} protocols\")\n",
    "print(f\"ðŸ“ˆ Metrics: {len(df_tab3)} protocols\")\n",
    "print(f\"ðŸŽ¯ Distribution: {len(df_tab4)} tokens\")\n",
    "if 'financial' in summary_stats:\n",
    "    fs = summary_stats['financial']\n",
    "    print(f\"\\nðŸ“ˆ Financial Summary:\")\n",
    "    print(f\" Daily Revenue: {format_currency(fs.get('total_daily_revenue', 0))}\")\n",
    "    print(f\" Daily Fees: {format_currency(fs.get('total_daily_fees', 0))}\")\n",
    "    print(f\" Avg P/F Ratio: {fs.get('avg_pf_ratio', 'N/A')}\")\n",
    "    print(f\" Avg P/R Ratio: {fs.get('avg_pr_ratio', 'N/A')}\")\n",
    "print(\"ðŸš€ Ready for Streamlit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c835c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
